---
layout: post
title: "[리눅스 응용] 모니터링 도구 (1)GPU편"
category: Linux
author: kira
create_date: "2023-07-29"
preview: 리눅스에서 GPU를 모니터링 할 수 있는 도구를 알아봅시다. 
image_url: "https://github.com/codelyst-blog/codelyst-blog.github.io/assets/54790133/f8efaf83-9312-4f23-9a19-e3b9e5bc9183"
---

<br> 

# Linux 모니터링 도구

## Contents
-----
1. [GPU 모니터링 도구: nvidia-smi](#1-gpu-모니터링-도구-nvidia-smi)
2. [nvidia-smi 자주 사용하는 옵션](#2-nivida-smi-자주-사용하는-옵션)

<br> 

## 1. GPU 모니터링 도구: nvidia-smi
- nvidia에서 제공하는 GPU Driver를 각 서비의 OS맞게 설치했다면, `nvidia-smi` 명령어를 통해 서버에 있는 GPU의 현재 모든 상태를 모니터링 할 수 있습니다.

    ![image](https://github.com/codelyst-blog/codelyst-blog.github.io/assets/54790133/ad413910-34e1-475b-b71a-e32e5be16cf0)

### `nvidia-smi`를 통해 확인할 수 있는 정보
1. `Drive Version`: 현재 설치되어 있는 드라이브 버전을 확인할 수 있습니다.
2. `CUDA Version`: GPU 드라이버 설치 당시 초기에 설치된 CUDA 버전으로, GPU 드라이버와 CUDA를 별도로 설치했다면, 현재 설치되어 있는 CUDA 버전과는 다를 수 있습니다(`nvcc -V` 명령어를 사용하여 cuda toolkit 설치 시 설치된 CUDA 버전을 확인할 수 있습니다)
3. `GPU/Fan`
    - GPU: 설치되어 있는 GPU 넘버를 말합니다
    - Fan: GPU 팬의 성능을 %로 나타낸 값입니다.(Tesla model을 사용하는 경우 팬이 따로 없기 때문에 값이 N/A(Not Available)로 뜨게 됩니다)
4. `Name/Temp`
    - Name: GPU 모델 네임이 뜨게 됩니다. 위와 같이 이름이 중간에 짤려서 안보이는 경우(GeForce ...), `nvidia-smi -q` 명령어를 사용하여 `Product Name` 에서 모델 풀 네임을 확인할 수 있습니다. 
5. `Perf(Performance)`: GPU의 성능을 나타내는 값으로, P0-P12까지 존재하며 P0에 가까울수록 성능이 좋습니다.
6. `Persistence-M/Pwr:Usage/Cap`
    - Persistence-M: Power limit을 설정 할 수 있는 값으로, on/off 두 가지 모드가 존재합니다. on 상태가 되면 limit을 설정할 수 있습니다.
    - Pwr:Usage/Cap: 현재 전력 사용량과 최대 용량입니다.
7. `Bus-Id`: 서버 제조사의 메인보드마다 가지고 있는 PCI slot에 부여된 BUS-Id입니다. 
8. `Disp.A/Memory-Usage`
    - Disp.A: 모니터 화면 출력을 해당 GPU로 하고 있는지 아닌지를 표시해줍니다. 모니터를 연결한 출력 포트의 GPU는 on 상태로 변경됩니다.
    - Memory-Usage: 현재 사용하는 GPU 메모리/사용가능한 총 메모리를 표시합니다.
9. `Volatile GPU-Util`: 1/6초 동안 사용된 GPU의 활용률로, 값이 높을 수록 GPU를 제대로 활용하고 있음을 말합니다.
10. `Uncorr.ECC/Compute M./MIG M.`
    - Uncorr.ECC: GPU가 데이터를 처리하는 과정에서 발생하는 에러들을 나타내는 수치입니다.
    - Compute M.: GPU의 공유 접근 방법을 표시하는 것으로, Default의 경우 여러 스레드가 해당 GPU를 동시에 공유하면서 사용할 수 있는 상태를 말합니다.
    - MIG M.: NVIDIa GPU A100에서부터 지원 가능한 값으로, GPU를 슬라이스하는 기능입니다.
11. `Processes`: GPU가 작업을 시작하기 전에는 "No running process found"가 뜨며, GPU가 작업을 시작하면 사진에서와 같이 PID, Process name, GPU Memory/Usage등이 뜨게 됩니다.


## 2. nivida-smi 자주 사용하는 옵션 
- 서버 모니터링 시 `nvidia-smi`에 다양한 옵션을 추가하여 사용하게 됩니다. 본 게시글에서는 두 가지 정도 소개드리고자 합니다

### nvidia-smi -q
- GPU와 Unit의 자세한 정보를 보여주는 옵션으로 `nvidia-smi`에서 짤리거나, 표시되지 않은 정보들을 여기서 더 자세하게 확인할 수 있습니다. 
- 위의 사진에서 짤린 GPU 모델의 풀네임을 확인하고 싶다면, 정규식 명령어인 `grep`을 사용하면 됩니다. 1대의 서버에 총 4개의 RTX 2080Ti GPU가 있는 것을 확인할 수 있습니다. 
    ![image.png](https://github.com/codelyst-blog/codelyst-blog.github.io/assets/54790133/6863911c-dac4-48e5-ada6-cb50a9281962)

### watch -d nvidia-smi
- 메모리 사용량과 GPU 활용량을 실시간으로 보고 싶을 때 쓰는 옵션으로, 이 명령어의 경우 2초마다 nvidia-smi에서 띄어주는 정보를 갱신해주게 됩니다. 변경된 값은 사진에서처럼 하이라이트 되어 표시됩니다.
    ![image.png](https://github.com/codelyst-blog/codelyst-blog.github.io/assets/54790133/f6168b69-a433-4802-a7a3-253f4161fdc0)

